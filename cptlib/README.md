These classes should provide a lot of the heavy lifting to scrape and parse
online property tax data with a minimum amount of code.

See [placer/scraper.py](../scrapers/placer/scraper.py) and
[placer/parser.py](../scrapers/placer/parser.py) for an example on how to use
these.

1. **Configure and instantiate the `Parcels` iterator**
Every scraper and parser will need to iterate through a list of Parcels, which
is often loaded from a CSV or a shapefile. The `ParcelsCSV` and
`ParcelsShapefile` class instances will provide the scraper and parser with
`Parcel` objects which will be duly scraped or parsed.
The `Parcels` subclasses are configurable by passing, e.g., the key for the APN
field and the CSV or Shapefile path. Gettng the lat/long from the CSV or
shapefile can be tricky because of the multiple formats that this may take so
for the centroid value you need to pass a function which will take the *row*
and return a centroid tuple. There are two functions (`centroidfn_from_latlng`
and `centroidfn_from_shape`) which will create the necessary functions for you.
Creating a `Parcels` instance looks like this:

```
parcels_gen = parcels.ParcelsShapefile('PL', 'APN', 'ADR1',
    parcels.centroidfn_from_shape(),
    os.path.join(DATA_DIR, 'Parcels.shp'))
parcels_gen.valid_apn_pattern = r'^\d{3}-\d{3}-\d{3}-\d{3}$'
```

In the above example we're:
* Loading a shapefile named `data/Parcels.shp`
* for Placer county
* where the APN is in the record with key `APN` and address has key `ADR1`
* and the location data exists as a polygon in the record, so we create a function
which will read it from the `points` key (the default) and return the centroid
* and APNs must match a particular regexp or be considered invalid and skipped

2. **Instantiate a `Scraper` with the `Parcels` instance**
The scraper will loop through each `Parcel` generated by the `Parcels` instance,
make a web request, and save the HTML to the data directory. You provide the
data directory path and a URL template for the web request. There are a number
of properties which can customize the web request behavior.

Creating a `Scraper` and running it looks like this:

```
scraper = scrapers.Scraper(parcels_gen, DATA_DIR,
    'https://common3.mptsweb.com/MBC/placer/tax/main/{apn_clean}/2020/0000')
scraper.request_unsuccessful_string = '<title>ERROR</title>'

scraper.scrape()
```

In the above example we're:
* Creating a `Scraper` using the `ParcelsShapefile` iterator we just created
* which will write to a directory structure in `data/`
* and request from a Placer-county website with the APN loaded into it
* and consider any responses with the HTML title 'ERROR' to be invalid

3. **Instantiate a `Parser`**
The parser will loop through each `Parcel` generated by the `Parcels` instance,
look for the local HTML file, parse it for tax info, and -- if found -- write
the `Parcel` information to the `output.csv` file.

Unlike `Scraper`s, parsing requires custom code so `Parser` is meant to be
overridden with a custom parsing function. However, the parent `Parser` handles
the looping and saving. The appropriate `Parser` subclass may already exist
for the flavor of HTML which you need to parse -- look in the [parsers.py] file.
For example, `ParserMegabyte` will parse mptsweb.com pages. If one doesn't exist
then you need to override the `_parse_html()` function, parse the `html` argument
and update the `parcel` instance.

Creating a `ParserMegabyte` instance and running it looks like this:

```
parser = parsers.ParserMegabyte(parcels_gen, DATA_DIR)
parser.parse()
```
